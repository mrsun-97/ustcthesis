@article{stewartEarlyHistorySingular1993,
	title = {On the {{Early History}} of the {{Singular Value Decomposition}}},
	author = {Stewart, G. W.},
	year = {1993},
	month = dec,
	volume = {35},
	pages = {551--566},
	publisher = {{Society for Industrial and Applied Mathematics}},
	issn = {0036-1445},
	doi = {10.1137/1035134},
	abstract = {This paper surveys the contributions of five mathematicians\textemdash Eugenio Beltrami (1835\textendash 1899), Camille Jordan (1838\textendash 1921), James Joseph Sylvester (1814\textendash 1897), Erhard Schmidt (1876\textendash 1959), and Hermann Weyl (1885\textendash 1955)\textemdash who were responsible for establishing the existence of the singular value decomposition and developing its theory.},
	file = {/home/parry/OneDrive/transfer_station/paper/SIAM Review/1993/Stewart_1993_On the Early History of the Singular Value Decomposition.pdf},
	journal = {SIAM Review},
	number = {4}
}

@article{eisertColloquiumAreaLaws2010,
	title = {Colloquium: {{Area}} Laws for the Entanglement Entropy},
	shorttitle = {Colloquium},
	author = {Eisert, J. and Cramer, M. and Plenio, M. B.},
	year = {2010},
	month = feb,
	volume = {82},
	pages = {277--306},
	publisher = {{American Physical Society}},
	doi = {10.1103/RevModPhys.82.277},
	abstract = {Physical interactions in quantum many-body systems are typically local: Individual constituents interact mainly with their few nearest neighbors. This locality of interactions is inherited by a decay of correlation functions, but also reflected by scaling laws of a quite profound quantity: the entanglement entropy of ground states. This entropy of the reduced state of a subregion often merely grows like the boundary area of the subregion, and not like its volume, in sharp contrast with an expected extensive behavior. Such ``area laws'' for the entanglement entropy and related quantities have received considerable attention in recent years. They emerge in several seemingly unrelated fields, in the context of black hole physics, quantum information science, and quantum many-body physics where they have important implications on the numerical simulation of lattice models. In this Colloquium the current status of area laws in these fields is reviewed. Center stage is taken by rigorous results on lattice models in one and higher spatial dimensions. The differences and similarities between bosonic and fermionic models are stressed, area laws are related to the velocity of information propagation in quantum lattice models, and disordered systems, nonequilibrium situations, and topological entanglement entropies are discussed. These questions are considered in classical and quantum systems, in their ground and thermal states, for a variety of correlation measures. A significant proportion is devoted to the clear and quantitative connection between the entanglement content of states and the possibility of their efficient numerical simulation. Matrix-product states, higher-dimensional analogs, and variational sets from entanglement renormalization are also discussed and the paper is concluded by highlighting the implications of area laws on quantifying the effective degrees of freedom that need to be considered in simulations of quantum states.},
	file = {/home/parry/OneDrive/transfer_station/paper/Reviews of Modern Physics/2010/Eisert et al_2010_Colloquium.pdf},
	journal = {Reviews of Modern Physics},
	number = {1}
}

@phdthesis{dongZhangLiangWangLuoSuanFaCongBoSeZiXiTongDaoFeiMiZiXiTong2017,
	title = {{张量网络算法从玻色子系统到费米子系统}},
	author = {董, 少钧},
	year = {2017},
	abstract = {量子强关联多体系统带来了一些在物理学中最令人兴奋的开放性问题。例如普遍认为Hubbard模型在高温超导中占有重要的角色;最近拓扑序,拓扑相吸引了广泛的兴趣。该领域的快速发展伴随着对相互作用的量子多体系统的理论认识的加深。一个用来描述量子多体系统的最新方法,就是张量网络态的方法。该方法通过抓住问题的关键,系统的基态空间只是由一些弱纠缠态组成的空间,于是把一个指数增长的问题变成一个多项式增长的问题,从而使得模拟大规模的量子系统成为可能。在本篇论文中,我们发展张量网络算法,并且利用张量网络算法研究了带阻挫隧穿作用的玻色Hubbard模型,主要完成了以下三方面的工作:1.完成了一个完整的张量网络函数库...},
	file = {/home/parry/Downloads/张量网络算法从玻色子系统到费米子系统_董少钧.caj;/home/parry/OneDrive/transfer_station/paper/中国科学技术大学/2017/董_2017_张量网络算法从玻色子系统到费米子系统.pdf},
	keywords = {Area Law,Condensed Matter Physics,Density Matrix Renormalization Group,Entanglement,Matrix Product States,Numerical Algorithms,Quantum Computation,Quantum Information,Strongly Correlated Many-body Systems,Tensor Network,Variational Principle,凝聚态物理,变分原理,多体强关联系统,密度矩阵重整化群,张量网络算法,数值算法,矩阵直积态,量子信息,量子纠缠,量子计算,面积定律},
	language = {中文;},
	school = {中国科学技术大学},
	type = {{博士}}
}

@article{hsiaoJournalClubBrief,
  title = {Journal {{Club}}: {{Brief Introduction}} to {{Tensor Network}}},
  author = {Hsiao, Wei-Han},
  pages = {7},
  file = {/home/parry/Zotero/storage/NBQ8HLYA/Hsiao - Journal Club Brief Introduction to Tensor Network.pdf},
  language = {en}
}

@article{PracticalIntroductionTensor2014,
	title = {A Practical Introduction to Tensor Networks: {{Matrix}} Product States and Projected Entangled Pair States},
	shorttitle = {A Practical Introduction to Tensor Networks},
	year = {2014},
	month = oct,
	volume = {349},
	pages = {117--158},
	publisher = {{Academic Press}},
	issn = {0003-4916},
	doi = {10.1016/j.aop.2014.06.013},
	abstract = {This is a partly non-technical introduction to selected topics on tensor network methods, based on several lectures and introductory seminars given on\ldots},
	file = {/home/parry/Zotero/storage/VD75D4SL/2014 - A practical introduction to tensor networks Matri.pdf;/home/parry/Zotero/storage/FNQDLI6R/S0003491614001596.html},
	journal = {Annals of Physics},
	language = {en}
}

@article{vidalEntanglementQuantumCritical2003,
	title = {Entanglement in {{Quantum Critical Phenomena}}},
	author = {Vidal, G. and Latorre, J. I. and Rico, E. and Kitaev, A.},
	year = {2003},
	month = jun,
	volume = {90},
	pages = {227902},
	publisher = {{American Physical Society}},
	doi = {10.1103/PhysRevLett.90.227902},
	abstract = {Entanglement, one of the most intriguing features of quantum theory and a main resource in quantum information science, is expected to play a crucial role also in the study of quantum phase transitions, where it is responsible for the appearance of long-range correlations. We investigate, through a microscopic calculation, the scaling properties of entanglement in spin chain systems, both near and at a quantum critical point. Our results establish a precise connection between concepts of quantum information, condensed matter physics, and quantum field theory, by showing that the behavior of critical entanglement in spin systems is analogous to that of entropy in conformal field theories. We explore some of the implications of this connection.},
	file = {/home/parry/OneDrive/transfer_station/paper/Physical Review Letters/2003/Vidal et al_2003_Entanglement in Quantum Critical Phenomena.pdf},
	journal = {Physical Review Letters},
	number = {22}
}

@article{eisertEntanglementTensorNetwork2013,
	title = {Entanglement and Tensor Network States},
	author = {Eisert, J.},
	year = {2013},
	month = sep,
	abstract = {These lecture notes provide a brief overview of methods of entanglement theory applied to the study of quantum many-body systems, as well as of tensor network states capturing quantum states naturally appearing in condensed-matter systems.},
	archivePrefix = {arXiv},
	eprint = {1308.3318},
	eprinttype = {arxiv},
	file = {/home/parry/Zotero/storage/YMS32Y22/Eisert - 2013 - Entanglement and tensor network states.pdf},
	journal = {arXiv:1308.3318 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Strongly Correlated Electrons,Quantum Physics},
	language = {en},
	primaryClass = {cond-mat, physics:quant-ph}
}

@article{bridgemanHandwavingInterpretiveDance2017,
	title = {Hand-Waving and Interpretive Dance: An Introductory Course on Tensor Networks},
	shorttitle = {Hand-Waving and Interpretive Dance},
	author = {Bridgeman, Jacob C. and Chubb, Christopher T.},
	year = {2017},
	month = may,
	volume = {50},
	pages = {223001},
	publisher = {{IOP Publishing}},
	issn = {1751-8121},
	doi = {10.1088/1751-8121/aa6dc3},
	abstract = {The curse of dimensionality associated with the Hilbert space of spin systems provides a significant obstruction to the study of condensed matter systems. Tensor networks have proven an important tool in attempting to overcome this difficulty in both the numerical and analytic regimes. These notes form the basis for a seven lecture course, introducing the basics of a range of common tensor networks and algorithms. In particular, we cover: introductory tensor network notation, applications to quantum information, basic properties of matrix product states, a classification of quantum phases using tensor networks, algorithms for finding matrix product states, basic properties of projected entangled pair states, and multiscale entanglement renormalisation ansatz states. The lectures are intended to be generally accessible, although the relevance of many of the examples may be lost on students without a background in many-body physics/quantum information. For each lecture, several problems are given, with worked solutions in an ancillary file.},
	file = {/home/parry/OneDrive/transfer_station/paper/Journal of Physics A Mathematical and Theoretical/2017/Bridgeman_Chubb_2017_Hand-waving and interpretive dance.pdf},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	language = {en},
	number = {22}
}


@article{osorioireguiInfiniteMatrixProduct2017,
	title = {Infinite Matrix Product States versus Infinite Projected Entangled-Pair States on the Cylinder: {{A}} Comparative Study},
	shorttitle = {Infinite Matrix Product States versus Infinite Projected Entangled-Pair States on the Cylinder},
	author = {Osorio Iregui, Juan and Troyer, Matthias and Corboz, Philippe},
	year = {2017},
	month = sep,
	volume = {96},
	pages = {115113},
	issn = {2469-9950, 2469-9969},
	doi = {10.1103/PhysRevB.96.115113},
	file = {/home/parry/Zotero/storage/2CMCWEV9/Osorio Iregui et al. - 2017 - Infinite matrix product states versus infinite pro.pdf},
	journal = {Physical Review B},
	language = {en},
	number = {11}
}

@article{lubaschUnifyingProjectedEntangled2014,
	title = {Unifying Projected Entangled Pair State Contractions},
	author = {Lubasch, Michael and Cirac, J. Ignacio and Ba{\~n}uls, Mari-Carmen},
	year = {2014},
	month = mar,
	volume = {16},
	pages = {033014},
	publisher = {{IOP Publishing}},
	issn = {1367-2630},
	doi = {10.1088/1367-2630/16/3/033014},
	abstract = {The approximate contraction of a tensor network of projected entangled pair states (PEPS) is a fundamental ingredient of any PEPS algorithm, required for the optimization of the tensors in ground state search or time evolution, as well as for the evaluation of expectation values. An exact contraction is in general impossible, and the choice of the approximating procedure determines the efficiency and accuracy of the algorithm. We analyze different previous proposals for this approximation, and show that they can be understood via the form of their environment, i.e. the operator that results from contracting part of the network. This provides physical insight into the limitation of various approaches, and allows us to introduce a new strategy, based on the idea of clusters, that unifies previous methods. The resulting contraction algorithm interpolates naturally between the cheapest and most imprecise and the most costly and most precise method. We benchmark the different algorithms with finite PEPS, and show how the cluster strategy can be used for both the tensor optimization and the calculation of expectation values. Additionally, we discuss its applicability to the parallelization of PEPS and to infinite systems.},
	file = {/home/parry/OneDrive/transfer_station/paper/New Journal of Physics/2014/Lubasch et al_2014_Unifying projected entangled pair state contractions.pdf},
	journal = {New Journal of Physics},
	language = {en},
	number = {3}
}


@article{schollwoeckDensitymatrixRenormalizationGroup2011,
	title = {The Density-Matrix Renormalization Group in the Age of Matrix Product States},
	author = {Schollwoeck, Ulrich},
	year = {2011},
	month = jan,
	volume = {326},
	pages = {96--192},
	issn = {00034916},
	doi = {10.1016/j.aop.2010.09.012},
	abstract = {The density-matrix renormalization group method (DMRG) has established itself over the last decade as the leading method for the simulation of the statics and dynamics of one-dimensional strongly correlated quantum lattice systems. In the further development of the method, the realization that DMRG operates on a highly interesting class of quantum states, so-called matrix product states (MPS), has allowed a much deeper understanding of the inner structure of the DMRG method, its further potential and its limitations. In this paper, I want to give a detailed exposition of current DMRG thinking in the MPS language in order to make the advisable implementation of the family of DMRG algorithms in exclusively MPS terms transparent. I then move on to discuss some directions of potentially fruitful further algorithmic development: while DMRG is a very mature method by now, I still see potential for further improvements, as exemplified by a number of recently introduced algorithms.},
	archivePrefix = {arXiv},
	eprint = {1008.3477},
	eprinttype = {arxiv},
	file = {/home/parry/OneDrive/transfer_station/paper/Annals of Physics/2011/Schollwoeck_2011_The density-matrix renormalization group in the age of matrix product states.pdf;/home/parry/Zotero/storage/B4RUBLVW/1008.html},
	journal = {Annals of Physics},
	keywords = {Condensed Matter - Strongly Correlated Electrons},
	number = {1}
}

@article{vidalClassicalSimulationInfiniteSize2007,
	title = {Classical {{Simulation}} of {{Infinite}}-{{Size Quantum Lattice Systems}} in {{One Spatial Dimension}}},
	author = {Vidal, G.},
	year = {2007},
	month = feb,
	volume = {98},
	pages = {070201},
	issn = {0031-9007, 1079-7114},
	doi = {10.1103/PhysRevLett.98.070201},
	file = {/home/parry/Zotero/storage/YHRR2CUY/Vidal - 2007 - Classical Simulation of Infinite-Size Quantum Latt.pdf},
	journal = {Physical Review Letters},
	language = {en},
	number = {7}
}

@article{liuGradientOptimizationFinite2017,
	title = {Gradient Optimization of Finite Projected Entangled Pair States},
	author = {Liu, Wen-Yuan and Dong, Shao-Jun and Han, Yong-Jian and Guo, Guang-Can and He, Lixin},
	year = {2017},
	month = may,
	volume = {95},
	pages = {195154},
	issn = {2469-9950, 2469-9969},
	doi = {10.1103/PhysRevB.95.195154},
	file = {/home/parry/Zotero/storage/6JCAIGJ9/Liu et al. - 2017 - Gradient optimization of finite projected entangle.pdf},
	journal = {Physical Review B},
	language = {en},
	number = {19}
}

@article{wangMonteCarloSimulation2011,
	title = {Monte {{Carlo}} Simulation with Tensor Network States},
	author = {Wang, Ling and Pi{\v z}orn, Iztok and Verstraete, Frank},
	year = {2011},
	month = apr,
	volume = {83},
	pages = {134421},
	issn = {1098-0121, 1550-235X},
	doi = {10.1103/PhysRevB.83.134421},
	file = {/home/parry/Zotero/storage/6KGJNVGS/Wang et al. - 2011 - Monte Carlo simulation with tensor network states.pdf},
	journal = {Physical Review B},
	language = {en},
	number = {13}
}

@article{sandvikVariationalQuantumMonte2007,
	title = {Variational {{Quantum Monte~Carlo Simulations}} with {{Tensor}}-{{Network States}}},
	author = {Sandvik, A. W.},
	year = {2007},
	volume = {99},
	doi = {10.1103/PhysRevLett.99.220602},
	file = {/home/parry/OneDrive/transfer_station/paper/Physical Review Letters/2007/Sandvik_2007_Variational Quantum Monte Carlo Simulations with Tensor-Network States.pdf;/home/parry/Zotero/storage/F6V8298G/PhysRevLett.99.html},
	journal = {Physical Review Letters},
	number = {22}
}

@article{bottouOptimizationMethodsLargeScale2018,
	title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
	author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
	year = {2018},
	month = feb,
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	archivePrefix = {arXiv},
	eprint = {1606.04838},
	eprinttype = {arxiv},
	file = {/home/parry/OneDrive/transfer_station/paper/arXiv1606.04838 [cs, math, stat]/2018/Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf;/home/parry/Zotero/storage/NIYRAFFF/1606.html},
	journal = {arXiv:1606.04838 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
	primaryClass = {cs, math, stat}
}


@article{johnsonAcceleratingStochasticGradient,
	title = {Accelerating {{Stochastic Gradient Descent}} Using {{Predictive Variance Reduction}}},
	author = {Johnson, Rie and Zhang, Tong},
	pages = {9},
	abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
	file = {/home/parry/Zotero/storage/GBQTA37I/Johnson and Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf},
	language = {en}
}

@article{rouxStochasticGradientMethod,
	title = {A {{Stochastic Gradient Method}} with an {{Exponential Convergence}} \_{{Rate}} for {{Finite Training Sets}}},
	author = {Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
	pages = {9},
	abstract = {We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.},
	file = {/home/parry/Zotero/storage/5SM7Z4Y3/Roux et al. - A Stochastic Gradient Method with an Exponential C.pdf},
	language = {en}
}

@article{hofmannVarianceReducedStochastic2016,
	title = {Variance {{Reduced Stochastic Gradient Descent}} with {{Neighbors}}},
	author = {Hofmann, Thomas and Lucchi, Aurelien and {Lacoste-Julien}, Simon and McWilliams, Brian},
	year = {2016},
	month = feb,
	abstract = {Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its slow convergence can be a computational bottleneck. Variance reduction techniques such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving linear convergence. However, these methods are either based on computations of full gradients at pivot points, or on keeping per data point corrections in memory. Therefore speed-ups relative to SGD may need a minimal number of epochs in order to materialize. This paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points, which offers advantages in the transient optimization phase. As a side-product we provide a unified convergence analysis for a family of variance reduction algorithms, which we call memorization algorithms. We provide experimental results supporting our theory.},
	archivePrefix = {arXiv},
	eprint = {1506.03662},
	eprinttype = {arxiv},
	file = {/home/parry/OneDrive/transfer_station/paper/arXiv1506.03662 [cs, math, stat]/2016/Hofmann et al_2016_Variance Reduced Stochastic Gradient Descent with Neighbors.pdf;/home/parry/Zotero/storage/YEE7Y7A5/1506.html},
	journal = {arXiv:1506.03662 [cs, math, stat]},
	keywords = {90C06; 90C25; 68T05,Computer Science - Machine Learning,G.1.6,I.2.6,Mathematics - Optimization and Control,Statistics - Machine Learning},
	primaryClass = {cs, math, stat}
}

@article{defazioSAGAFastIncremental2014,
	title = {{{SAGA}}: {{A Fast Incremental Gradient Method With Support}} for {{Non}}-{{Strongly Convex Composite Objectives}}},
	shorttitle = {{{SAGA}}},
	author = {Defazio, Aaron and Bach, Francis and {Lacoste-Julien}, Simon},
	year = {2014},
	month = dec,
	abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
	archivePrefix = {arXiv},
	eprint = {1407.0202},
	eprinttype = {arxiv},
	file = {/home/parry/OneDrive/transfer_station/paper/arXiv1407.0202 [cs, math, stat]/2014/Defazio et al_2014_SAGA.pdf;/home/parry/Zotero/storage/WZ2YHDVT/1407.html},
	journal = {arXiv:1407.0202 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
	primaryClass = {cs, math, stat}
}

@misc{ErWeiLiangZiDuoTiXiTongDeZhangLiangWangLuoTaiSuanFaZhongGuoBoShiXueWeiLunWenQuanWenShuJuKu,
	title = {二维量子多体系统的张量网络态算法 - 中国博士学位论文全文数据库},
	author = {Wenyuan Liu},
	year = {2017},
	month = may,
	file = {/home/parry/Zotero/storage/LYU9LBSJ/二维量子多体系统的张量网络态算法 - 中国博士学位论文全文数据库.pdf;/home/parry/Zotero/storage/STBR4BTZ/detail.html},
	howpublished = {http://gb.oversea.cnki.net/KCMS/detail/detail.aspx?filename=1017071080.nh\&dbcode=CDFD\&dbname=CDFDREF}
}

@InProceedings{pmlr-v54-lei17a,
	title = 	 {{Less than a Single Pass: Stochastically Controlled Stochastic Gradient}},
	author = 	 {Lihua Lei and Michael Jordan},
	booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {148--156},
	year = 	 {2017},
	editor = 	 {Aarti Singh and Jerry Zhu},
	volume = 	 {54},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Fort Lauderdale, FL, USA},
	month = 	 {20--22 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v54/lei17a/lei17a.pdf},
	url = 	 {http://proceedings.mlr.press/v54/lei17a.html},
	abstract = 	 {We develop and analyze a procedure for gradient-based optimization that we refer to as stochastically controlled stochastic gradient (SCSG). As a member of the SVRG family of algorithms, SCSG makes use of gradient estimates at two scales. Unlike most existing algorithms in this family, both the computation cost and the communication cost of SCSG do not necessarily scale linearly with the sample size n; indeed, these costs are independent of n when the target accuracy is small. An experimental evaluation of SCSG on the MNIST dataset shows that it can yield accurate results on this dataset on a single commodity machine with a memory footprint of only 2.6MB and only eight disk accesses.}
}


@article{vedralMeanfieldApproximationsMultipartite2004,
	title = {Mean-Field Approximations and Multipartite Thermal Correlations},
	author = {Vedral, Vlatko},
	year = {2004},
	month = feb,
	volume = {6},
	pages = {22--22},
	issn = {1367-2630},
	doi = {10.1088/1367-2630/6/1/022},
	abstract = {The relationship between the mean-field approximations in various interacting models of statistical physics and measures of classical and quantum correlations is explored. We present a method that allows us to find an upper bound for the total amount of correlations (and hence entanglement) in a physical system in thermal equilibrium at some temperature in terms of its free energy and internal energy. This method is first illustrated by using two qubits interacting through the Heisenberg coupling, where entanglement and correlations can be computed exactly. It is then applied to the one-dimensional (1D) Ising model in a transverse magnetic field, for which entanglement and correlations cannot be obtained by exact methods. We analyse the behaviour of correlations in various regimes and identify critical regions, comparing them with already known results. Finally, we present a general discussion of the effects of entanglement on the macroscopic, thermodynamical features of solid-state systems. In particular, we exploit the fact that a d-dimensional quantum system in thermal equilibrium can be made to correspond to a (d + 1)-dimensional classical system in equilibrium to substitute all entanglement for classical correlations.},
	file = {/home/parry/Zotero/storage/ZE3S2J57/Vedral - 2004 - Mean-field approximations and multipartite thermal.pdf},
	journal = {New Journal of Physics},
	language = {en}
}

@book{QMC1999,
	title = "Quantum Monte Carlo methods in physics and chemistry",
	author = "M.P Nightingale and C.J Umrigar",
	year = "1999",
	language = "Dutch",
	volume = "154",
	publisher = "Springer",
	
}

@book{2012lanczos,
	title = {Lanczos Algorithms for Large Symmetric Eigenvalue Computations Vol. {{II}} Programs},
	year = {2012},
	publisher = {{Birkh\"auser Boston}},
	isbn = {978-1-4684-9178-4},
	series = {Progress in Scientific Computing}
}

@book{dengDensityFunctionalTheory2014,
	title = {{Density functional theory(Chinese Edition)}},
	author = {DENG, MEI ] SI TE KE ER},
	year = {2014},
	month = sep,
	publisher = {{Defense Industry Press}},
	address = {{北京}},
	abstract = {Language:Chinese.Pub Date: 2014-09-01 Pages: 238 Publisher: National Defense Industry Press Hsiao. Steckel edited this density functional theory is calculated using a large number of instances. particularly for plane wave DFT calculations DFT calculations. carried out in simple terms concise introduction. This book not only describes some of the key concepts of the DFT. but the isolation of the DFT state molecule. practical application of solid bulk. surface and interface aspects of a systematic and comprehensive explanation. Make the reader in learning DFT calculations. can mainly concentrate on modeling and solving practical problems. try to avoid too much time spent on the basic theory of quantum mechanics. solid state physics. such as the esoteric. This is a reference book on DFT calculations very good entry-learning materials and practical calculations. both for beginne...},
	isbn = {978-7-118-09737-5},
	language = {Chinese}
}


